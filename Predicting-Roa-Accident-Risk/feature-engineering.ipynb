{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942eeb90-cee9-41c2-9c54-84eb4a5c391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "df_preprocessed = pd.get_dummies(df, drop_first=True, dtype=int).astype(float)\n",
    "X = df_preprocessed.drop(['accident_risk', 'id'], axis=1)\n",
    "y = df_preprocessed['accident_risk']\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003275c6-155b-44f5-944a-fdad8855609a",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "I would train a the full data set on three base models:\n",
    "- LightGBM\n",
    "- XGBoost\n",
    "- RandomForest\n",
    "And check on these that the model will increase or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b74eff5-ddf6-4cba-897c-110a586e42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0356772-58aa-4da7-91d3-a57cef81d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df_preprocessed = pd.get_dummies(df, drop_first=True, dtype=int).astype(float)\n",
    "X = df_preprocessed.drop(['accident_risk', 'id'], axis=1)\n",
    "y = df_preprocessed['accident_risk']\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b84d719-0b9e-4854-bf4b-d4772dd49438",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_parameters = {\n",
    "    'num_leaves': 171,\n",
    "    'max_depth': 74,\n",
    "    'learning_rate': 0.017777738697293582,\n",
    "    'n_estimators': 486,\n",
    "}\n",
    "\n",
    "xgboost_parameters = {\n",
    "    'max_leaves': 209,\n",
    "    'max_depth': 167,\n",
    "    'learning_rate': 0.02215636350717474,\n",
    "    'n_estimators': 348,\n",
    "    'device': 'cuda',\n",
    "}\n",
    "\n",
    "randomforest_parameters = {\n",
    "    'n_estimators': 256,\n",
    "    'max_depth': 23,\n",
    "    'min_samples_split': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features': 'log2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4bd0c8e-88f8-422b-a702-1c5409572cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_obj, parameters, kf, X, y):    \n",
    "    rmses = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        train_data = X.iloc[train_index]\n",
    "        test_data = X.iloc[test_index]\n",
    "\n",
    "        train_output = y.iloc[train_index]\n",
    "        test_output = y.iloc[test_index]\n",
    "\n",
    "        model = model_obj(**parameters)\n",
    "        model.fit(train_data, train_output)\n",
    "\n",
    "        y_pred = model.predict(test_data)\n",
    "\n",
    "        rmse = root_mean_squared_error(y_pred, test_output)\n",
    "        rmses.append(rmse)\n",
    "\n",
    "    return np.mean(rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3348e1bb-8dbd-4d5b-8417-b729ed4618fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002461 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 176\n",
      "[LightGBM] [Info] Number of data points in the train set: 414203, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.352414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 171\n",
      "[LightGBM] [Info] Number of data points in the train set: 414203, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.352093\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 177\n",
      "[LightGBM] [Info] Number of data points in the train set: 414203, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.352568\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 175\n",
      "[LightGBM] [Info] Number of data points in the train set: 414203, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.352341\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 172\n",
      "[LightGBM] [Info] Number of data points in the train set: 414204, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.352471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LGBM': np.float64(0.056037552103227695),\n",
       " 'XGB': np.float64(0.05604319910312414),\n",
       " 'RF': np.float64(0.057142978032406665)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np \n",
    "\n",
    "kf = KFold(5, shuffle=True)\n",
    "\n",
    "base_models_cv = {\n",
    "    'LGBM': evaluate_model(LGBMRegressor, lightgbm_parameters, kf, X, y),\n",
    "    'XGB': evaluate_model(XGBRegressor, xgboost_parameters, kf, X, y),\n",
    "    'RF': evaluate_model(RandomForestRegressor, randomforest_parameters, kf, X, y),\n",
    "}\n",
    "base_models_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d93ff-06a2-4395-addd-e800f67bd010",
   "metadata": {},
   "source": [
    "## Check out datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "343cef38-c6c5-4d84-af5b-31563b05b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "base_risk = (\n",
    "    0.3 * df[\"curvature\"] + \n",
    "    0.2 * (df[\"lighting\"] == \"night\").astype(int) + \n",
    "    0.1 * (df[\"weather\"] != \"clear\").astype(int) + \n",
    "    0.2 * (df[\"speed_limit\"] >= 60).astype(int) + \n",
    "    0.1 * (np.array(df[\"num_reported_accidents\"]) > 2).astype(int)\n",
    ")\n",
    "df['Meta'] = base_risk\n",
    "\n",
    "df_preprocessed = pd.get_dummies(df, drop_first=True, dtype=int).astype(float)\n",
    "X = df_preprocessed.drop(['accident_risk', 'id'], axis=1)\n",
    "y = df_preprocessed['accident_risk']\n",
    "\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "base_risk = (\n",
    "    0.3 * test_df[\"curvature\"] + \n",
    "    0.2 * (test_df[\"lighting\"] == \"night\").astype(int) + \n",
    "    0.1 * (test_df[\"weather\"] != \"clear\").astype(int) + \n",
    "    0.2 * (test_df[\"speed_limit\"] >= 60).astype(int) + \n",
    "    0.1 * (np.array(test_df[\"num_reported_accidents\"]) > 2).astype(int)\n",
    ")\n",
    "test_df['Meta'] = base_risk\n",
    "test_df_preprocessed = pd.get_dummies(test_df, drop_first=True, dtype=int).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a82a7a3c-5933-4e36-9867-0b36adedbd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_preprocessed.to_csv('data/test_pp.csv', index=False)\n",
    "df_preprocessed.to_csv('data/train_pp.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
